{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "979b6f8823bbd681f0a04e070b770d1a",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homewok 1 ML\n",
    "## Warning: \n",
    "### Don't even try to add new Import or you will marked 0 point\n",
    "### Don't make an infinite loop or you or you will marked 0 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put your npm as string below\n",
    "NPM = \"1506726510\"\n",
    "\n",
    "#if someone that also take an ML class this year help you doing this assingment,put his/her NPM as list of string below\n",
    "COLLABORATORS = [\"1506758052\", \"1506727160\"]\n",
    "\n",
    "#if youre using external sources please put its link as list of string below\n",
    "SOURCES = [\"https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4668bf8743c05b811c5d03a5fc6115d8",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21a67f669f8cc8a0d97ceb97556ba2f4",
     "grade": false,
     "grade_id": "helper_func",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def is_similar(X, Y):\n",
    "    if isinstance(X, list) and isinstance(Y, list):\n",
    "        if len(X) == len(Y):\n",
    "            return all([is_similar(x,Y[i]) for i,x in enumerate(X)])\n",
    "        else:\n",
    "            return False\n",
    "    return abs(X-Y) <= 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40c65eee2c40c843125e2074cfbf409d",
     "grade": true,
     "grade_id": "import_check_helper",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6e452e5999e2d507915805f16265a98d",
     "grade": false,
     "grade_id": "header_linreg",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f99b99903004b5eb5eb2a883dcbdc551",
     "grade": false,
     "grade_id": "hx_quest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### score: 1 point\n",
    "without using any external library, implement h(x)\n",
    "\n",
    "$$ h(x) = \\theta^Tx $$\n",
    "\n",
    "with thetas as $\\theta$ (represented as list)\n",
    "\n",
    "x represented as list with the same length with thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8bc4be778d7df98094b52982e87ab79c",
     "grade": false,
     "grade_id": "hx_ans",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def hx(x):\n",
    "    global theta\n",
    "    return sum(i*j for i,j in zip(theta,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "49cdd85e487934efc9ce8090c6cae440",
     "grade": true,
     "grade_id": "hx_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta, x = [1,2,3], [2,3,4]\n",
    "assert is_similar(hx(x),20)\n",
    "\n",
    "theta, x = [-1,-1,-1], [2,3,4]\n",
    "assert is_similar(hx(x),-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "809a9f446d3003ff64281be4c3892844",
     "grade": false,
     "grade_id": "JOfTheta_quest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### score: 1 point\n",
    "\n",
    "without using any external library, using hx implemented above, implement $J(\\theta)$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "y as list of target value\n",
    "\n",
    "with X as list of list of example coresponding to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c3d49190a179b55da2b38f719cbf1484",
     "grade": false,
     "grade_id": "JOfTheta_ans",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cost_func(X, y):\n",
    "    m = len(y)\n",
    "    j=  0\n",
    "    for i in range (m):\n",
    "        j+=(hx(X[i])-y[i])**2\n",
    "    j=j/2\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98d43d25d33f536e2896eff7c9d6857f",
     "grade": true,
     "grade_id": "JOfTheta_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta, X, y = [-1, 0], [[1,1],[0,1]], [-1,0]\n",
    "assert is_similar(cost_func(X,y),0)\n",
    "\n",
    "theta, X, y = [1, 1, 2], [[1,1,1],[0,0,0]], [0,0]\n",
    "assert is_similar(cost_func(X,y),8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b2eb755912c468c627e02840277bf59d",
     "grade": false,
     "grade_id": "sgd_quest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### score: 1 point\n",
    "without using any external library, implement one iteration of stochastic gradient descent that use cost function above as error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dd2e4433daf1b56ea5a78e86e5cc11ae",
     "grade": false,
     "grade_id": "sgd_ans",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def one_iteration_of_SGD(X, y, lr=1e-4):\n",
    "    global theta\n",
    "    alpha, m = lr, len(y)\n",
    "    for i in range(m):\n",
    "        mult = alpha*(y[i]-hx(X[i]))\n",
    "        arr = [0]*len(X[i])\n",
    "        for j in range(len(X[i])):\n",
    "            x = X[i][j]\n",
    "            arr[j] = x*mult\n",
    "        for j in range(len(theta)):\n",
    "            theta[j] = theta[j]+arr[j]\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d462257663781eedb12724d22c1ec8d1",
     "grade": true,
     "grade_id": "sgd_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta, X, y = [1, 2, 1], [],[]\n",
    "# optimal theta is [0,1,-1]\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        X.append([1,i,j])\n",
    "        y.append(i-j)\n",
    "theta = one_iteration_of_SGD(X, y, lr=0.02)\n",
    "assert is_similar(theta,[0.026437395230493967, 1.0273911806101215, -0.9817775908722853])\n",
    "theta = one_iteration_of_SGD(X, y, lr=0.02)\n",
    "assert is_similar(theta,[0.008423488895377855, 0.9997426554553306, -0.9996855620207235])\n",
    "theta = one_iteration_of_SGD(X, y, lr=0.02)\n",
    "assert is_similar(theta,[0.0070527414486398375, 0.9990690600988367, -1.0001740371160284])\n",
    "theta = one_iteration_of_SGD(X, y, lr=0.02)\n",
    "assert is_similar(theta,[0.006252955113893083, 0.999152947934675, -1.0001675531854075])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost func before 12575.0\n",
      "theta(weight) before [1, 2, 1]\n",
      "cost func after 20 iteration 1.0169437719700162e-05\n",
      "theta(weight) after 20 iteration [0.0009404489777786889, 0.9998724942098921, -1.0000252664652551]\n",
      "\n",
      "cost/loss func plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mcongratulations you have implemented linear regression from scratch \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# PLAYGROUND\n",
    "# you can do anything here as long as not adding any new import\n",
    "\n",
    "theta, X, y = [1, 2, 1], [],[]\n",
    "# optimal theta is [0,1,-1]\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        X.append([1,i,j])\n",
    "        y.append(i-j)\n",
    "\n",
    "history = []\n",
    "print('cost func before', cost_func(X,y))\n",
    "print('theta(weight) before', theta)\n",
    "for i in range(20):\n",
    "    history.append(cost_func(X,y)) \n",
    "    theta = one_iteration_of_SGD(X, y, lr=0.02)\n",
    "print('cost func after 20 iteration', cost_func(X,y))\n",
    "print('theta(weight) after 20 iteration', theta)\n",
    "\n",
    "print('\\ncost/loss func plot')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history)\n",
    "plt.xlabel('number of iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print('\\033[1mcongratulations you have implemented linear regression from scratch \\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d083efe63ab16adf9b1269c42140fa5",
     "grade": false,
     "grade_id": "header_glm",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# GLM (Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cd4b53ec4de05217534f21f8de9f104a",
     "grade": false,
     "grade_id": "softmax_quest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### score: 1 point\n",
    "without using any external library, implement a softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0f9df8ea2547434d4937effd690ffd02",
     "grade": false,
     "grade_id": "softmax_ans",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    e = 2.718281828459045\n",
    "    exps = [e**i for i in Z]\n",
    "    sum_of_exps = sum(exps)\n",
    "    softmax = [j/sum_of_exps for j in exps]\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "37e5e77d76a2c04e0e78a0cde1454ffb",
     "grade": true,
     "grade_id": "softmax_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert is_similar(softmax([1]), [1])\n",
    "assert is_similar(softmax([100]), [1])\n",
    "assert is_similar(softmax([0.123]), [1])\n",
    "assert is_similar(softmax([1, 1]), [0.5, 0.5])\n",
    "assert is_similar(softmax([123, 123, 123, 123]), [0.25, 0.25, 0.25, 0.25])\n",
    "assert is_similar(softmax([10, 1]), [0.9998766054240137, 0.0001233945759862318])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "668a8f05585e62c7535eb7540a3d8f01",
     "grade": false,
     "grade_id": "softmax_reg_quest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### score: 1 point\n",
    "without using any external library, given theta/weight of a layer and x as features vector implement a single input softmax regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e0225694db12b7cfccd4ff448d332d86",
     "grade": false,
     "grade_id": "softmax_reg_ans",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def single_softmax_regression(x):\n",
    "    global thetas\n",
    "    e = 2.718281828459045\n",
    "        \n",
    "    m = len(thetas)\n",
    "    softmaxs = [0]*m\n",
    "    sum_val = 0\n",
    "    \n",
    "    def dot_product(A,B):\n",
    "        res = 0\n",
    "        m = len(A)\n",
    "        for i in range(m):\n",
    "            res += A[i]*B[i]\n",
    "        return res\n",
    "    \n",
    "    for i in range(m):\n",
    "        sum_val+=e**(dot_product(thetas[i],x))\n",
    "        \n",
    "    for i in range(m):\n",
    "        softmaxs[i] = (e**dot_product(thetas[i],x))/sum_val\n",
    "    return softmaxs\n",
    "\n",
    "def softmax_regression(X):\n",
    "    return [single_softmax_regression(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0e3d483d60b13cb44c3a215273059714",
     "grade": true,
     "grade_id": "softmax_reg_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "thetas = [[0,0],[0,0]]\n",
    "assert is_similar(softmax_regression([[1,0],[0,1]]),[[0.5, 0.5], [0.5, 0.5]])\n",
    "thetas = [[1,0], [0,1]]\n",
    "assert is_similar(softmax_regression([[1,0],[0,1]]),[[0.7310585786300049, 0.2689414213699951], [0.2689414213699951, 0.7310585786300049]])\n",
    "thetas = [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "assert is_similar(softmax_regression([[1, 1, 1, 2], [1,2, 1, 3]]),[[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]])\n",
    "thetas = [[0, 0, 0, 100], [1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "assert is_similar(softmax_regression([[1, 1, 1, 1], [0,0, 1, 0], [100,0,0,1]]),[[1, 0, 0, 0], [0.25, 0.25, 0.25, 0.25], [0.5, 0.5, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f18d0a2302f85b97237eb12de3afa62f",
     "grade": false,
     "grade_id": "softmax_predict_quest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### score: 1 point\n",
    "without using any external library, given X as matrics of features(column) and datapoint(row) output its one hot encoded predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5e560d52d016ff93d829b69c85ee3ec8",
     "grade": false,
     "grade_id": "softmax_predict_ans",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X):\n",
    "    likelyhoods = softmax_regression(X)\n",
    "    classes = [[0 for i in range(len(x))] for x in X]\n",
    "    L = softmax_regression(X)\n",
    "    for LL in L:\n",
    "        m = len(LL)\n",
    "        mx = max(LL)\n",
    "        for i in range(m):\n",
    "            if LL[i]<mx:\n",
    "                LL[i] = 0\n",
    "            else:\n",
    "                LL[i] = 1\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70a0c1c62116cbd649f59c2f79a6c26e",
     "grade": true,
     "grade_id": "softmax_predict_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "thetas = [[1,0], [0,1]]\n",
    "assert predict_class([[1,0],[0,1]]) == [[1, 0], [0, 1]]\n",
    "thetas = [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "\n",
    "thetas = [[0, 0, 0, 100], [1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "assert predict_class([[1, 1, 1, 1]]) == [[1, 0, 0, 0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f10dc985702ac86eef82d986168cbe1",
     "grade": true,
     "grade_id": "import_check_1",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d36c1670517f68a39153710d68aa4d0a",
     "grade": false,
     "grade_id": "header_gda",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7bec1b281bea281fc2e24b55ad1c4e6c",
     "grade": true,
     "grade_id": "import_checker_helper_1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e550b7767b70d29a732be7eb461bb784",
     "grade": false,
     "grade_id": "gda_quest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### total score: 2 point\n",
    "##### 1 point for fit (correct sigma, mu0, mu1,  phi)\n",
    "##### 1 point for predict (correct compute_predict_proba_x_given_y,  compute_proba_y, predict_single)\n",
    "\n",
    "implement Gaussian Discriminant Analysis model for binary classification problem **you are only allowed to use numpy and numpy.linalg**\n",
    "\n",
    "use function fit to train the model\n",
    "\n",
    "and function predict to predict the trained model\n",
    "\n",
    "X is matrix of data (column as features, row as datapoint)\n",
    "\n",
    "y is vector of label (0 or 1) coresponding to X\n",
    "\n",
    "phi is $\\phi$\n",
    "\n",
    "mu0 is $\\mu0$\n",
    "\n",
    "mu1 is $\\mu1$\n",
    "\n",
    "sigma/covmat is $\\Sigma$\n",
    "\n",
    "### IMPLEMENTATION DETAIL\n",
    "**USE 1/(m-1) instead of 1/m for sigma/covmat or you can just use np.cov**\n",
    "\n",
    "**USE np.linalg.pinv instead of np.linalg.inv for inverse matrix (pseudo inverse instead of inverse)**\n",
    "\n",
    "**USE np.pi for pi**\n",
    "\n",
    "**USE np.det to get determinant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "74781002ae680f61d875a27dabeda76a",
     "grade": false,
     "grade_id": "gda_ans",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class GDA:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.phi = None\n",
    "        self.sigma = None\n",
    "        self.mu0 = None\n",
    "        self.mu1 = None\n",
    "    \n",
    "    # update phi, sigma, mu0(mean 0), mu1(mean 1) here\n",
    "    def fit(self, X, y):\n",
    "        m = y.shape[0]\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        self.mus = [self.mu0, self.mu1]\n",
    "\n",
    "    \n",
    "    # return p(y)\n",
    "    def compute_proba_y(self, y):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    # return p(x|y)\n",
    "    def compute_proba_x_given_y(self,x, y):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        proba_y0 = self.compute_proba_y(y=0)\n",
    "        proba_y1 = self.compute_proba_y(y=1)\n",
    "        \n",
    "        proba_x_given_y_0 = self.compute_proba_x_given_y(x, y=0)\n",
    "        proba_x_given_y_1 = self.compute_proba_x_given_y(x, y=1)\n",
    "        return 1 if proba_y1*proba_x_given_y_1 > proba_y0*proba_x_given_y_0 else 0\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return [self.predict_single(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6543a820a015a31df675ef47ff52535f",
     "grade": true,
     "grade_id": "gda_test_fit",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_gda_public():\n",
    "    X = np.zeros((1000,3))\n",
    "    np.random.seed(103)\n",
    "    X[0:500,0] = np.random.uniform(-3,1,500)\n",
    "    np.random.seed(47)\n",
    "    X[0:500,1] = np.random.uniform(-3,1,500)\n",
    "    np.random.seed(2019)\n",
    "    X[0:500,2] = np.random.uniform(-3,1,500)\n",
    "\n",
    "    np.random.seed(103)\n",
    "    X[500:,0] = np.random.uniform(-1,3,500)\n",
    "    np.random.seed(47)\n",
    "    X[500:,1] = np.random.uniform(-1,3,500)\n",
    "    np.random.seed(2019)\n",
    "    X[500:,2] = np.random.uniform(-3,1,500)\n",
    "    y = np.array([0 if i < 500 else 1 for i in range(1000)])\n",
    "\n",
    "    gda = GDA()\n",
    "    gda.fit(X,y)\n",
    "    return X, y, gda\n",
    "\n",
    "X,y, gda = test_gda_public()\n",
    "np.testing.assert_almost_equal(gda.phi, 0.5)\n",
    "np.testing.assert_array_almost_equal(gda.mu0, [-1.0170375245138221, -0.8837860293254437, -0.9928110497167374])\n",
    "np.testing.assert_array_almost_equal(gda.mu1, [0.982962475486178, 1.1162139706745566, -0.9928110497167374])\n",
    "np.testing.assert_array_almost_equal(gda.sigma, np.array([[ 1.31469689, -0.06407885, -0.03699664], [-0.06407885,  1.36877185, -0.04577584], [-0.03699664, -0.04577584,  1.3349875 ]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "393f2cf39cbe441f3e191fb48293b119",
     "grade": true,
     "grade_id": "gda_test_proba",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X,y, gda = test_gda_public()\n",
    "\n",
    "np.testing.assert_almost_equal(gda.compute_proba_y(1), 0.5)\n",
    "np.testing.assert_almost_equal(gda.compute_proba_y(0), 0.5)\n",
    "\n",
    "np.testing.assert_almost_equal(gda.compute_proba_x_given_y([-3, -3, -3], 1), 0)\n",
    "np.testing.assert_almost_equal(gda.compute_proba_x_given_y([-3, -3, -3], 0), 0.0002746507453048722)\n",
    "np.testing.assert_array_almost_equal(gda.predict([[-3,-3,-3]]), [0])\n",
    "\n",
    "np.testing.assert_almost_equal(gda.compute_proba_x_given_y([3, 3, 1], 1), 0.0003846724892885247)\n",
    "np.testing.assert_almost_equal(gda.compute_proba_x_given_y([3, 3, 1], 0), 0)\n",
    "np.testing.assert_array_almost_equal(gda.predict([[3, 3, 1]]), [1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYGROUND\n",
    "# you can do anything here as long as not adding any new import or breaking stuff\n",
    "\n",
    "X = np.zeros((1000,2))\n",
    "\n",
    "# for targe label 0\n",
    "np.random.seed(103)\n",
    "X[0:500,0] = np.random.uniform(-3,1,500)\n",
    "np.random.seed(47)\n",
    "X[0:500,1] = np.random.uniform(-3,1,500)\n",
    "\n",
    "# for targe label 1\n",
    "np.random.seed(103)\n",
    "X[500:,0] = np.random.uniform(-1,3,500)\n",
    "np.random.seed(47)\n",
    "X[500:,1] = np.random.uniform(-1,3,500)\n",
    "y = np.array([0 if i < 500 else 1 for i in range(1000)])\n",
    "\n",
    "\n",
    "gda = GDA()\n",
    "gda.fit(X,y)\n",
    "\n",
    "Z0 = [np.exp(-0.5*np.dot(np.dot((x-gda.mu0),np.linalg.pinv(gda.sigma)),(x-gda.mu0))) for x in X]\n",
    "Z1 = [np.exp(-0.5*np.dot(np.dot((x-gda.mu1),np.linalg.pinv(gda.sigma)),(x-gda.mu1))) for x in X]\n",
    "\n",
    "def plot_countour(X,Z0,Z1,y):\n",
    "    # define grid.\n",
    "    xi = np.linspace(-5,5,100)\n",
    "    yi = np.linspace(-5,5,100)\n",
    "\n",
    "    zi0 = griddata((X[:,0], X[:,1]), Z0, (xi[None,:], yi[:,None]), method='cubic')\n",
    "    zi1 = griddata((X[:,0], X[:,1]), Z1, (xi[None,:], yi[:,None]), method='cubic')    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    CS = plt.contour(xi,yi,zi0,10,linewidths=0.5,colors='red')\n",
    "    CS = plt.contour(xi,yi,zi1,10,linewidths=0.5,colors='blue')\n",
    "    \n",
    "    plt.scatter(X[np.where(y==0)[0]][:,0],X[np.where(y==0)[0]][:,1],marker='o',c='red', s=1)\n",
    "    plt.scatter(X[np.where(y==1)[0]][:,0],X[np.where(y==1)[0]][:,1],marker='x',c='blue',s=1)\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "    plt.title('gda boundary plot')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('example where GDA works well')\n",
    "plot_countour(X,np.array(Z0), np.array(Z1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "578a2b5e1a398fcbc7a3a4f49ad19f40",
     "grade": true,
     "grade_id": "import_check_2",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes Classifier\n",
    "\n",
    "#### score 2 (you have to implement everything correctly to get 2 score, no partial scoring, you got either 2 or 0)\n",
    "implement a Multinomial Naive Bayes Classifier for text classification using laplace smoothing\n",
    "\n",
    "X is list of list of tokenized sentence\n",
    "\n",
    "y is the target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0cbd6908772501b19d6aded5cdc544b4",
     "grade": true,
     "grade_id": "import_check_helper_2",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "eed60034254615965d26da0743dde4f4",
     "grade": false,
     "grade_id": "mnb_ans",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MNB_TextClassifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prior = {}\n",
    "        self.condprob = {}\n",
    "    \n",
    "    # p(class=y | {term0=x[0], term1=x[1], ...} )\n",
    "    def proba_y_given_x(self, y, x):\n",
    "        proaba_y = self.proba_y(y)\n",
    "        for var in x:\n",
    "            if var in self.condprob:\n",
    "                proaba_y *= self.condprob[var][y]\n",
    "            else:\n",
    "                proaba_y *= self.condprob['not_in_vocabularies'][y]\n",
    "        return proaba_y\n",
    "    \n",
    "    # p(c)\n",
    "    def proba_y(self, y):\n",
    "        return self.prior[y]\n",
    "\n",
    "    # update self.prior[class] as p(class=class) \n",
    "    # update self.condprob[term][class] as p(term=term | class=class)\n",
    "    def fit(self, X, y):\n",
    "        #calculate p(class=class)\n",
    "        y_uniques, y_counts = np.unique(y, return_counts=True)\n",
    "        for i in range (len(y_uniques)):\n",
    "            probability_y = y_counts[i]/float(y.size)\n",
    "            self.prior[y_uniques[i]]=probability_y\n",
    "        \n",
    "        \n",
    "        #calculate p(term|class)\n",
    "        vocabularies =  set(x for l in X for x in l)\n",
    "        class_count = {}\n",
    "        vocab_count = {}\n",
    "        inner_condprob = {}\n",
    "        for val in vocabularies:\n",
    "            self.condprob[val] = {}\n",
    "                \n",
    "        class_count = class_count.fromkeys(self.prior, 0)\n",
    "            \n",
    "        for i in range (len(y_uniques)):\n",
    "            for j in range(len(X)):\n",
    "                if (y_uniques[i] == y[j]):\n",
    "                    class_count[y_uniques[i]] += len(X[j])\n",
    "                    \n",
    "        for i in range (len(y_uniques)):\n",
    "            vocab_count = vocab_count.fromkeys(vocabularies, 0)\n",
    "            for j in range(len(X)):\n",
    "                if(y_uniques[i] == y[j]):\n",
    "                    for k in range (len(X[j])):\n",
    "                        vocab_count[X[j][k]] += 1\n",
    "            for key, value in vocab_count.items():\n",
    "                proaba = (value+1)/(class_count[i]+len(vocabularies))\n",
    "                self.condprob[key][y_uniques[i]] = proaba\n",
    "                \n",
    "                \n",
    "        self.condprob['not_in_vocabularies'] = {}\n",
    "        for i in range (len(y_uniques)):        \n",
    "            self.condprob['not_in_vocabularies'][y_uniques[i]] = 1/(class_count[i]+len(vocabularies))\n",
    "            \n",
    "        return 0\n",
    "        \n",
    "    def predict_single(self, x):\n",
    "        result = dict(self.prior)\n",
    "        for key in self.prior:\n",
    "            result[key] = self.proba_y_given_x(key,x)\n",
    "            \n",
    "        return max(result, key=result.get)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return [self.predict_single(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "05e0465b7049b93df482c2fe2b246488",
     "grade": true,
     "grade_id": "mnb_test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    ['chinese', 'beijing', 'chinese'],\n",
    "    ['chinese', 'chinese', 'shanghai'],\n",
    "    ['chinese', 'macao'],\n",
    "    ['tokyo', 'japan', 'chinese']\n",
    "])\n",
    "\n",
    "#class 1 is china\n",
    "#class 0 is japan\n",
    "y = np.array([1,1,1,0])\n",
    "\n",
    "clf = MNB_TextClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "np.testing.assert_almost_equal(clf.condprob['chinese'][1], 3/7)\n",
    "np.testing.assert_almost_equal(clf.condprob['japan'][1], 1/14)\n",
    "np.testing.assert_almost_equal(clf.condprob['chinese'][0], 2/9)\n",
    "np.testing.assert_almost_equal(clf.condprob['japan'][0], 2/9)\n",
    "\n",
    "np.testing.assert_almost_equal(clf.proba_y_given_x(1, ['asdf']), 0.05357142857142857)\n",
    "\n",
    "np.testing.assert_almost_equal(clf.proba_y_given_x(1, ['chinese', 'chinese', 'chinese', 'tokyo', 'japan']), 0.00030121377997263036)\n",
    "np.testing.assert_almost_equal(clf.proba_y_given_x(1, ['asdfasdfasdf','asdfasdfasdf','chinese', 'chinese', 'chinese', 'tokyo', 'japan']), 1.5368049998603588e-06)\n",
    "\n",
    "np.testing.assert_almost_equal(clf.proba_y_given_x(0, ['chinese', 'chinese', 'chinese', 'tokyo', 'japan']), 0.00013548070246744226)\n",
    "\n",
    "\n",
    "np.testing.assert_almost_equal(clf.proba_y(1), 3/4)\n",
    "np.testing.assert_almost_equal(clf.proba_y(0), 1/4)\n",
    "np.testing.assert_almost_equal(clf.predict_single(['chinese', 'chinese', 'tokyo', 'japan']), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6de7e48f74c582a3e3039826a29ad115",
     "grade": true,
     "grade_id": "import_check_3",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "baf2bbdb60b1df8cd6679eb4c3af1c75",
     "grade": false,
     "grade_id": "cell-f3b7ede3d1085e13",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## OPTIONAL (NO SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give us your feedback of your experience on taking this test here\n",
    "FEEDBACK = \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
